{
	"name": "Transformation_Dims_Customer_Copy1",
	"properties": {
		"folder": {
			"name": "Transformations"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "LCsparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "cff4e15a-00cb-4466-a329-5b6891dca6b6"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5e8b7639-1039-4c46-a250-81622382b0f1/resourceGroups/RG_lendingClub/providers/Microsoft.Synapse/workspaces/ws-lendingclub/bigDataPools/LCsparkpool",
				"name": "LCsparkpool",
				"type": "Spark",
				"endpoint": "https://ws-lendingclub.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LCsparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Query to find the total count of customers by state and country"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Add required imports\r\n",
					"import com.microsoft.spark.sqlanalytics\r\n",
					"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
					"from pyspark.sql.functions import avg, count,col,when,countDistinct,round, filter"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"# Read from existing internal table\r\n",
					"df_customer_dim = (spark.read\r\n",
					"                     .option(Constants.TEMP_FOLDER, \"abfss://fs-lendingclub-staging@lendingclubadlsgen2.dfs.core.windows.net/temp_staging\")\r\n",
					"                     .synapsesql(\"LC_DWH.gold.customer_dim\")\r\n",
					"                     # Column-pruning i.e., query select column values.\r\n",
					"                    )\r\n",
					"\r\n",
					"# Show contents of the dataframe\r\n",
					"df_customer_dim.show()"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_customer_dim.createOrReplaceTempView(\"customer_details\")"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Query to find the average age of customers by state and country using pyspark dataframe"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Query to find the average age of customers by state and country using pyspark dataframe\r\n",
					"customer_avg_age=spark.sql(\"SELECT state, country, count(*) as total_count FROM customer_details GROUP BY state, country\")"
				],
				"execution_count": 48
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"customer_avg_age.show()"
				],
				"execution_count": 49
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Query to find the number of customers with a premium status of \"true\" in each country, grouped by age range"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Query to find the number of customers with a premium status of \"true\" in each country, grouped by age range using pyspark dataframe:\r\n",
					"\r\n",
					"customer_age_range=spark.sql(\"SELECT country, CASE WHEN age BETWEEN 18 AND 25 THEN 'Youngsters' \\\r\n",
					"           WHEN age BETWEEN 26 AND 35 THEN 'Working class' \\\r\n",
					"           WHEN age BETWEEN 36 AND 45 THEN 'Middle Age' \\\r\n",
					"           ELSE 'Senior Citizens' \\\r\n",
					"       END as age_range, \\\r\n",
					"       COUNT(*) \\\r\n",
					" FROM customer_details \\\r\n",
					"WHERE prm_status = 'TRUE' \\\r\n",
					"GROUP BY country, age_range\")"
				],
				"execution_count": 53
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"customer_age_range.show()"
				],
				"execution_count": 54
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Query to find the percentage of customers in each state that are premium customers, grouped by country"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# --Query to find the percentage of customers in each state that are premium customers, grouped by country:\r\n",
					"\r\n",
					"member_percentage_df = spark.sql(\" \\\r\n",
					"WITH customer_counts AS ( \\\r\n",
					"    SELECT country, state, COUNT(*) as total_customers \\\r\n",
					"    FROM customer_details \\\r\n",
					"    GROUP BY country, state \\\r\n",
					"), \\\r\n",
					"member_counts AS ( \\\r\n",
					"    SELECT country, state, COUNT(DISTINCT member_id) as total_members \\\r\n",
					"    FROM customer_details \\\r\n",
					"    WHERE member_id IS NOT NULL and  prm_status = 'TRUE' \\\r\n",
					"    GROUP BY country, state \\\r\n",
					") \\\r\n",
					"SELECT customer_counts.country, customer_counts.state, \\\r\n",
					"       ROUND(member_counts.total_members / customer_counts.total_customers * 100, 2) as member_percentage \\\r\n",
					"FROM customer_counts \\\r\n",
					"JOIN member_counts \\\r\n",
					"ON customer_counts.country = member_counts.country AND customer_counts.state = member_counts.state \\\r\n",
					" \")"
				],
				"execution_count": 56
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"member_percentage_df.show()"
				],
				"execution_count": 57
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Read from existing internal table\r\n",
					"df_payment_details_dim = (spark.read\r\n",
					"                     .option(Constants.TEMP_FOLDER, \"abfss://fs-lendingclub-staging@lendingclubadlsgen2.dfs.core.windows.net/temp_staging\")\r\n",
					"                     .synapsesql(\"LC_DWH.gold.Payment_Details_Dim\")\r\n",
					"                     # Column-pruning i.e., query select column values.\r\n",
					"                    )\r\n",
					"\r\n",
					"# Show contents of the dataframe\r\n",
					"df_payment_details_dim.show()"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_payment_details_dim.createOrReplaceTempView(\"payment_details\")"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.conf.set(\"spark.sql.unacceptable_rated_pts\",0)\r\n",
					"spark.conf.set(\"spark.sql.very_bad_rated_pts\",100)\r\n",
					"spark.conf.set(\"spark.sql.bad_rated_pts\",250)\r\n",
					"spark.conf.set(\"spark.sql.good_rated_pts\",500)\r\n",
					"spark.conf.set(\"spark.sql.very_good_rated_pts\",650)\r\n",
					"spark.conf.set(\"spark.sql.excellent_rated_pts\",800)"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.conf.set(\"spark.sql.unacceptable_grade_pts\",750)\r\n",
					"spark.conf.set(\"spark.sql.very_bad_grade_pts\", 1000)\r\n",
					"spark.conf.set(\"spark.sql.bad_grade_pts\",1500)\r\n",
					"spark.conf.set(\"spark.sql.good_grade_pts\",2000)\r\n",
					"spark.conf.set(\"spark.sql.very_good_grade_pts\",2500)"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"unacceptable_grade=\"F\"\r\n",
					"very_bad_grade=\"E\"\r\n",
					"bad_grade=\"D\"\r\n",
					"good_grade=\"C\"\r\n",
					"very_good_grade=\"B\"\r\n",
					"excellent_grade=\"A\""
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"payment_last_df = spark.sql(\"select c.member_id, c.state, c.country, c.first_name, c.last_name, \\\r\n",
					"  case \\\r\n",
					"    when p.last_pymnt_amnt < (p.installment * 0.5) then ${spark.sql.very_bad_rated_pts} \\\r\n",
					"    when p.last_pymnt_amnt >= (p.installment * 0.5) and p.last_pymnt_amnt < p.installment then ${spark.sql.bad_rated_pts} \\\r\n",
					"    when (p.last_pymnt_amnt = (p.installment)) then ${spark.sql.good_rated_pts} \\\r\n",
					"    when p.last_pymnt_amnt > (p.installment) and p.last_pymnt_amnt <= (p.installment * 1.50) then ${spark.sql.very_good_rated_pts} \\\r\n",
					"    when p.last_pymnt_amnt > (p.installment * 1.50) then ${spark.sql.excellent_rated_pts} \\\r\n",
					"    else ${spark.sql.unacceptable_rated_pts} \\\r\n",
					"  end as last_payment_pts, \\\r\n",
					"  case \\\r\n",
					"    when p.total_pymnt >= (p.funded_amnt_inv * 0.50) then ${spark.sql.very_good_rated_pts} \\\r\n",
					"    when p.total_pymnt < (p.funded_amnt_inv * 0.50) and p.total_pymnt > 0 then ${spark.sql.good_rated_pts} \\\r\n",
					"    when p.total_pymnt = 0 or (p.total_pymnt) is null then ${spark.sql.unacceptable_rated_pts} \\\r\n",
					"    end as total_payment_pts \\\r\n",
					"from payment_details p \\\r\n",
					"inner join customer_details c on c.member_id = p.member_id\")"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"payment_last_df.createOrReplaceTempView(\"payment_points_df\")"
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sql(\"select * from payment_points_df where last_payment_pts!= 500 or total_payment_pts!=500 \").show(7)"
				],
				"execution_count": 19
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Reading data from Loan Defaulter Dimension Table "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Read from existing internal table\r\n",
					"df_loan_defaulter_dim = (spark.read\r\n",
					"                     .option(Constants.TEMP_FOLDER, \"abfss://fs-lendingclub-staging@lendingclubadlsgen2.dfs.core.windows.net/temp_staging\")\r\n",
					"                     .synapsesql(\"LC_DWH.gold.Loan_Defaulter_Dim\")\r\n",
					"                     # Column-pruning i.e., query select column values.\r\n",
					"                    )\r\n",
					"\r\n",
					"# Show contents of the dataframe\r\n",
					""
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"display(df_loan_defaulter_dim.show(5))"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_loan_defaulter_dim.createOrReplaceTempView(\"loan_default_details\")"
				],
				"execution_count": 22
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Assigning point metrics"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"loan_default_pts = spark.sql(\r\n",
					"    \"SELECT p.*, \\\r\n",
					"    CASE \\\r\n",
					"    WHEN l.delinq_2yrs = 0 THEN ${spark.sql.excellent_rated_pts} \\\r\n",
					"    WHEN l.delinq_2yrs BETWEEN 1 AND 2 THEN ${spark.sql.bad_rated_pts} \\\r\n",
					"    WHEN l.delinq_2yrs BETWEEN 3 AND 5 THEN ${spark.sql.very_bad_rated_pts} \\\r\n",
					"    WHEN l.delinq_2yrs > 5 OR l.delinq_2yrs IS NULL THEN ${spark.sql.unacceptable_rated_pts} \\\r\n",
					"    END AS delinq_pts, \\\r\n",
					"    CASE \\\r\n",
					"    WHEN l.pub_rec = 0 THEN ${spark.sql.excellent_rated_pts} \\\r\n",
					"    WHEN l.pub_rec BETWEEN 1 AND 2 THEN ${spark.sql.bad_rated_pts} \\\r\n",
					"    WHEN l.pub_rec BETWEEN 3 AND 5 THEN ${spark.sql.very_bad_rated_pts} \\\r\n",
					"    WHEN l.pub_rec > 5 OR l.pub_rec IS NULL THEN ${spark.sql.very_bad_rated_pts} \\\r\n",
					"    END AS pub_rec_pts, \\\r\n",
					"    CASE \\\r\n",
					"    WHEN l.pub_rec_bankruptcies = 0 THEN ${spark.sql.excellent_rated_pts}  \\\r\n",
					"    WHEN l.pub_rec_bankruptcies BETWEEN 1 AND 2 THEN ${spark.sql.bad_rated_pts} \\\r\n",
					"    WHEN l.pub_rec_bankruptcies BETWEEN 3 AND 5 THEN ${spark.sql.very_bad_rated_pts} \\\r\n",
					"    WHEN l.pub_rec_bankruptcies > 5 OR l.pub_rec_bankruptcies IS NULL THEN ${spark.sql.unacceptable_rated_pts} \\\r\n",
					"    END AS public_bankruptcies_pts, \\\r\n",
					"    CASE \\\r\n",
					"    WHEN l.inq_last_6mths = 0 THEN ${spark.sql.excellent_rated_pts} \\\r\n",
					"    WHEN l.inq_last_6mths BETWEEN 1 AND 2 THEN ${spark.sql.bad_rated_pts} \\\r\n",
					"    WHEN l.inq_last_6mths BETWEEN 3 AND 5 THEN ${spark.sql.very_bad_rated_pts} \\\r\n",
					"    WHEN l.inq_last_6mths > 5 OR l.inq_last_6mths IS NULL THEN ${spark.sql.unacceptable_rated_pts} \\\r\n",
					"    END AS enq_pts, \\\r\n",
					"    CASE \\\r\n",
					"    WHEN l.hardship_flag = 'N' THEN ${spark.sql.very_good_rated_pts} \\\r\n",
					"    WHEN l.hardship_flag = 'Y' OR l.hardship_flag IS NULL THEN ${spark.sql.bad_rated_pts} \\\r\n",
					"    END AS hardship_pts \\\r\n",
					"    FROM loan_default_details l \\\r\n",
					"    LEFT JOIN payment_points_df p ON p.member_id = l.member_id\"\r\n",
					")"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"loan_default_pts.createOrReplaceTempView(\"loan_default_points_df\")"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Read from existing internal table\r\n",
					"loan_df = (spark.read\r\n",
					"                     .option(Constants.TEMP_FOLDER, \"abfss://fs-lendingclub-staging@lendingclubadlsgen2.dfs.core.windows.net/temp_staging\")\r\n",
					"                     .synapsesql(\"LC_DWH.gold.Loan_Details_Dim\")\r\n",
					"                     # Column-pruning i.e., query select column values.\r\n",
					"                    )\r\n",
					"\r\n",
					"# Show contents of the dataframe\r\n",
					"loan_df.show(5)"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"loan_df.createOrReplaceTempView(\"loan_details\")"
				],
				"execution_count": 26
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Reading data from Accounts Details Dimension Table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"# Read from existing internal table\r\n",
					"account_df = (spark.read\r\n",
					"                     .option(Constants.TEMP_FOLDER, \"abfss://fs-lendingclub-staging@lendingclubadlsgen2.dfs.core.windows.net/temp_staging\")\r\n",
					"                     .synapsesql(\"LC_DWH.gold.Account_Details_Dim\")\r\n",
					"                     # Column-pruning i.e., query select column values.\r\n",
					"                    )\r\n",
					"\r\n",
					"# Show contents of the dataframe\r\n",
					"account_df.show(5)"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"account_df.createOrReplaceTempView(\"account_details\")"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Read from existing internal table\r\n",
					"df_account_details_dim = (spark.read\r\n",
					"                     .option(Constants.TEMP_FOLDER, \"abfss://fs-lendingclub-staging@lendingclubadlsgen2.dfs.core.windows.net/temp_staging\")\r\n",
					"                     .synapsesql(\"LC_DWH.gold.Account_Details_Dim\")\r\n",
					"                     # Column-pruning i.e., query select column values.\r\n",
					"                    )\r\n",
					"\r\n",
					"# Show contents of the dataframe\r\n",
					"df_account_details_dim.show(5)"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"dfToReadFromTable.createOrReplaceTempView(\"customer_details\")"
				],
				"execution_count": 30
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Calculating Loan Scores"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"financial_df = spark.sql(\r\n",
					"    \"SELECT ldef.*, \\\r\n",
					"    CASE \\\r\n",
					"        WHEN LOWER(l.loan_status) LIKE '%fully paid%' THEN ${spark.sql.excellent_rated_pts} \\\r\n",
					"        WHEN LOWER(l.loan_status) LIKE '%current%' THEN ${spark.sql.good_rated_pts} \\\r\n",
					"        WHEN LOWER(l.loan_status) LIKE '%in grace period%' THEN ${spark.sql.bad_rated_pts} \\\r\n",
					"        WHEN LOWER(l.loan_status) LIKE '%late (16-30 days)%' OR LOWER(l.loan_status) LIKE '%late (31-120 days)%' THEN ${spark.sql.very_bad_rated_pts} \\\r\n",
					"        WHEN LOWER(l.loan_status) LIKE '%charged off%' THEN ${spark.sql.unacceptable_rated_pts} \\\r\n",
					"    END AS loan_status_pts, \\\r\n",
					"    CASE \\\r\n",
					"        WHEN LOWER(a.home_ownership) LIKE '%own%' THEN ${spark.sql.excellent_rated_pts} \\\r\n",
					"        WHEN LOWER(a.home_ownership) LIKE '%rent%' THEN ${spark.sql.good_rated_pts} \\\r\n",
					"        WHEN LOWER(a.home_ownership) LIKE '%mortgage%' THEN ${spark.sql.bad_rated_pts} \\\r\n",
					"        WHEN LOWER(a.home_ownership) LIKE '%any%' OR LOWER(a.home_ownership) IS NULL THEN ${spark.sql.very_bad_rated_pts} \\\r\n",
					"    END AS home_pts,  \\\r\n",
					"    CASE \\\r\n",
					"        WHEN l.funded_amnt <= (a.tot_hi_cred_lim * 0.10) THEN ${spark.sql.excellent_rated_pts}  \\\r\n",
					"        WHEN l.funded_amnt > (a.tot_hi_cred_lim * 0.10) AND l.funded_amnt <= (a.tot_hi_cred_lim * 0.20)  THEN ${spark.sql.very_good_rated_pts}  \\\r\n",
					"        WHEN l.funded_amnt > (a.tot_hi_cred_lim * 0.20) AND l.funded_amnt <= (a.tot_hi_cred_lim * 0.30)  THEN ${spark.sql.good_rated_pts}  \\\r\n",
					"        WHEN l.funded_amnt > (a.tot_hi_cred_lim * 0.30) AND l.funded_amnt <= (a.tot_hi_cred_lim * 0.50)  THEN ${spark.sql.bad_rated_pts}  \\\r\n",
					"        WHEN l.funded_amnt > (a.tot_hi_cred_lim * 0.50) AND l.funded_amnt <= (a.tot_hi_cred_lim * 0.70)  THEN ${spark.sql.very_bad_rated_pts}  \\\r\n",
					"        WHEN l.funded_amnt > (a.tot_hi_cred_lim * 0.70) THEN ${spark.sql.unacceptable_rated_pts}  \\\r\n",
					"    END AS credit_limit_pts, \\\r\n",
					"    CASE \\\r\n",
					"        WHEN (a.grade) ='A' and (a.sub_grade)='A1' THEN ${spark.sql.excellent_rated_pts}  \\\r\n",
					"        WHEN (a.grade) ='A' and (a.sub_grade)='A2' THEN (${spark.sql.excellent_rated_pts}* 0.80)  \\\r\n",
					"        WHEN (a.grade) ='A' and (a.sub_grade)='A3' THEN (${spark.sql.excellent_rated_pts}* 0.60)  \\\r\n",
					"        WHEN (a.grade) ='A' and (a.sub_grade)='A4' THEN (${spark.sql.excellent_rated_pts}* 0.40)  \\\r\n",
					"        WHEN (a.grade) ='A' and (a.sub_grade)='A5' THEN (${spark.sql.excellent_rated_pts}* 0.20)  \\\r\n",
					"        WHEN (a.grade) ='B' and (a.sub_grade)='B1' THEN (${spark.sql.very_good_rated_pts})  \\\r\n",
					"        WHEN (a.grade) ='B' and (a.sub_grade)='B2' THEN (${spark.sql.very_good_rated_pts}* 0.80)  \\\r\n",
					"        WHEN (a.grade) ='B' and (a.sub_grade)='B3' THEN (${spark.sql.very_good_rated_pts}* 0.60)  \\\r\n",
					"        WHEN (a.grade) ='B' and (a.sub_grade)='B4' THEN (${spark.sql.very_good_rated_pts}* 0.40)  \\\r\n",
					"        WHEN (a.grade) ='B' and (a.sub_grade)='B5' THEN (${spark.sql.very_good_rated_pts}* 0.20)  \\\r\n",
					"        WHEN (a.grade) ='C' and (a.sub_grade)='C1' THEN (${spark.sql.good_rated_pts})  \\\r\n",
					"        WHEN (a.grade) ='C' and (a.sub_grade)='C2' THEN (${spark.sql.good_rated_pts}* 0.80)  \\\r\n",
					"        WHEN (a.grade) ='C' and (a.sub_grade)='C3' THEN (${spark.sql.good_rated_pts}* 0.60)  \\\r\n",
					"        WHEN (a.grade) ='C' and (a.sub_grade)='C4' THEN (${spark.sql.good_rated_pts}* 0.40)  \\\r\n",
					"        WHEN (a.grade) ='C' and (a.sub_grade)='C5' THEN (${spark.sql.good_rated_pts}* 0.20)  \\\r\n",
					"        WHEN (a.grade) ='D' and (a.sub_grade)='D1' THEN (${spark.sql.bad_rated_pts})  \\\r\n",
					"        WHEN (a.grade) ='D' and (a.sub_grade)='D2' THEN (${spark.sql.bad_rated_pts}*0.80)  \\\r\n",
					"        WHEN (a.grade) ='D' and (a.sub_grade)='D3' THEN (${spark.sql.bad_rated_pts}*0.60)  \\\r\n",
					"        WHEN (a.grade) ='D' and (a.sub_grade)='D4' THEN (${spark.sql.bad_rated_pts}*0.40)  \\\r\n",
					"        WHEN (a.grade) ='D' and (a.sub_grade)='D5' THEN (${spark.sql.bad_rated_pts}*0.20)  \\\r\n",
					"        WHEN (a.grade) ='E' and (a.sub_grade)='E1' THEN (${spark.sql.very_bad_rated_pts})  \\\r\n",
					"        WHEN (a.grade) ='E' and (a.sub_grade)='E2' THEN (${spark.sql.very_bad_rated_pts}*0.80)  \\\r\n",
					"        WHEN (a.grade) ='E' and (a.sub_grade)='E3' THEN (${spark.sql.very_bad_rated_pts}*0.60)  \\\r\n",
					"        WHEN (a.grade) ='E' and (a.sub_grade)='E4' THEN (${spark.sql.very_bad_rated_pts}*0.40)  \\\r\n",
					"        WHEN (a.grade) ='E' and (a.sub_grade)='E5' THEN (${spark.sql.very_bad_rated_pts}*0.20)  \\\r\n",
					"        WHEN (a.grade) in ('F','G') and (a.sub_grade) in ('F1','G1') THEN (${spark.sql.unacceptable_rated_pts})  \\\r\n",
					"        WHEN (a.grade) in ('F','G') and (a.sub_grade) in ('F2','G2') THEN (${spark.sql.unacceptable_rated_pts}*0.80)  \\\r\n",
					"        WHEN (a.grade) in ('F','G') and (a.sub_grade) in ('F3','G3') THEN (${spark.sql.unacceptable_rated_pts}*0.60)  \\\r\n",
					"        WHEN (a.grade) in ('F','G') and (a.sub_grade) in ('F4','G4') THEN (${spark.sql.unacceptable_rated_pts}*0.40)  \\\r\n",
					"        WHEN (a.grade) in ('F','G') and (a.sub_grade) in ('F5','G5') THEN (${spark.sql.unacceptable_rated_pts}*0.20)  \\\r\n",
					"    END AS grade_pts \\\r\n",
					" FROM loan_default_points_df ldef \\\r\n",
					" LEFT JOIN loan_details l ON ldef.member_id = l.member_id \\\r\n",
					" LEFT JOIN account_details a ON a.member_id = ldef.member_id\")"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"financial_df.show()"
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"financial_df.printSchema()"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"financial_df.createOrReplaceTempView(\"loan_score_details\")"
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"loan_score = spark.sql(\"SELECT member_id, first_name, last_name, state, country, \\\r\n",
					"((last_payment_pts+total_payment_pts)*0.20) as payment_history_pts, \\\r\n",
					"((delinq_pts +pub_rec_pts+public_bankruptcies_pts+enq_pts+hardship_pts)*0.45) as defaulters_history_pts, \\\r\n",
					"((loan_status_pts+home_pts+credit_limit_pts+grade_pts)*0.35) as financial_health_pts \\\r\n",
					"FROM loan_score_details\")"
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"loan_score.show(5)"
				],
				"execution_count": 40
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"loan_score.createOrReplaceTempView(\"loan_score_pts\")"
				],
				"execution_count": 41
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"loan_score_final=spark.sql(\"select ls.member_id,ls.first_name,ls.last_name,ls.state,ls.country, \\\r\n",
					"(payment_history_pts+defaulters_history_pts+financial_health_pts) as loan_score \\\r\n",
					"from loan_score_pts ls \")"
				],
				"execution_count": 42
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"loan_score_final.createOrReplaceTempView(\"loan_score_eval\")"
				],
				"execution_count": 43
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"loan_score_final=spark.sql(\"select ls.*, \\\r\n",
					"case \\\r\n",
					"WHEN loan_score > ${spark.sql.very_good_grade_pts} THEN '\" + excellent_grade + \"' \\\r\n",
					"WHEN loan_score <= ${spark.sql.very_good_grade_pts} AND loan_score > ${spark.sql.good_grade_pts} THEN '\" + very_good_grade + \"' \\\r\n",
					"WHEN loan_score <= ${spark.sql.good_grade_pts} AND loan_score > ${spark.sql.bad_grade_pts} THEN '\" + good_grade + \"' \\\r\n",
					"WHEN loan_score <= ${spark.sql.bad_grade_pts} AND loan_score > ${spark.sql.very_bad_grade_pts} THEN '\" + bad_grade + \"' \\\r\n",
					"WHEN loan_score <= ${spark.sql.very_bad_grade_pts} AND loan_score > ${spark.sql.unacceptable_grade_pts} THEN '\" + very_bad_grade + \"' \\\r\n",
					"WHEN loan_score <= ${spark.sql.unacceptable_grade_pts} THEN '\" + unacceptable_grade + \"' \\\r\n",
					"end as loan_final_grade \\\r\n",
					"from loan_score_eval ls\")"
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"loan_score_final.createOrReplaceTempView(\"loan_final_table\")"
				],
				"execution_count": 45
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"loan_score_final.show(10)"
				],
				"execution_count": 46
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sql(\"select * from loan_final_table where loan_final_grade in ('C') \").show()"
				],
				"execution_count": 47
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# (df.write\r\n",
					"#  # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
					"#  # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
					"#  # .option(Constants.SERVER, \"lendingclubwspace.sql.azuresynapse.net\")\r\n",
					"#  # if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
					"#  .option(Constants.TEMP_FOLDER, \"abfss://fs-lendingclub-staging@lendingclubadlsgen2.dfs.core.windows.net/temp_staging\")\r\n",
					"#  .mode(\"overwrite\")\r\n",
					"#  # Required parameter - Three-part table name to which data will be written\r\n",
					"#  .synapsesql(\"LC_DWH.gold.customer_dim\", Constants.INTERNAL))"
				],
				"execution_count": 36
			}
		]
	}
}